{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from tensorflow.python.framework import ops\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from scipy import ndimage\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01]\n",
      "[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x=np.random.rand(4)\n",
    "np.random.seed(1)\n",
    "y=np.random.rand(4)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y,n_labels):\n",
    "    return np.eye(n_labels)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(isSave):\n",
    "    \n",
    "    # isSave will be true when we want to save the data in pkl and will be false when \n",
    "    # we want to get the data from pkl\n",
    "    if isSave:\n",
    "        classes=os.listdir(\"leaf_dataset\")\n",
    "\n",
    "        n_class=len(classes)\n",
    "        samples_per_class=16\n",
    "        image_width=50 # width of image\n",
    "        image_height=50 # height of image\n",
    "        n_channel=1 # number of channels\n",
    "        m=n_class*samples_per_class # Number of samples))\n",
    "\n",
    "        train_ratio=0.7\n",
    "        validation_ratio=0.2\n",
    "        test_ratio=0.1\n",
    "\n",
    "        ntraining_samples=int(samples_per_class*train_ratio)\n",
    "        nvalidation_samples=int(samples_per_class*validation_ratio)\n",
    "        ntest_samples=samples_per_class-ntraining_samples-nvalidation_samples\n",
    "        \n",
    "\n",
    "        print(\"m\",m)\n",
    "        print(\"classes\",len(classes))\n",
    "        print(\"ntraining_samples\",ntraining_samples)\n",
    "        print(\"nvalidation_samples\",nvalidation_samples)\n",
    "        print(\"ntest_samples\",ntest_samples)\n",
    "\n",
    "        x_train_set=np.zeros((ntraining_samples*n_class,image_width,image_height,n_channel),dtype=np.float32)\n",
    "        x_validation_set=np.zeros((nvalidation_samples*n_class,image_width,image_height,n_channel),dtype=np.float32)\n",
    "        x_test_set=np.zeros((ntest_samples*n_class,image_width,image_height,n_channel),dtype=np.float32)\n",
    "\n",
    "        y_train_set=np.zeros((ntraining_samples*n_class),dtype=int)\n",
    "        y_validation_set=np.zeros((nvalidation_samples*n_class),dtype=int)\n",
    "        y_test_set=np.zeros((ntest_samples*n_class),dtype=int)\n",
    "\n",
    "        train_count=0\n",
    "        test_count=0\n",
    "        validation_count=0\n",
    "        \n",
    "        print(\"Data is Saving .....\")\n",
    "\n",
    "        for count,class_name in enumerate(classes):\n",
    "            class_name_path=\"leaf_dataset/\"+class_name+\"/\"\n",
    "\n",
    "            images=os.listdir(class_name_path);\n",
    "            random.shuffle(images)\n",
    "\n",
    "            train_images=images[:ntraining_samples]\n",
    "            validation_images=images[ntraining_samples:ntraining_samples+nvalidation_samples]\n",
    "            test_images=images[ntraining_samples+nvalidation_samples:]\n",
    "\n",
    "#             print(\"train_images\",train_images)\n",
    "#             print(\"validation_images\",validation_images)\n",
    "#             print(\"test_images\",test_images)\n",
    "\n",
    "            # Filling training set\n",
    "            for image_name in train_images:\n",
    "    #             print(\"image_name\",image_name)\n",
    "                image=img.imread(class_name_path+image_name)\n",
    "    #             print(image.shape)\n",
    "                image=cv2.resize(image, dsize=(image_height,image_width), interpolation=cv2.INTER_CUBIC).reshape(image_height,image_width,n_channel)/255\n",
    "    #             print(image.shape)\n",
    "                x_train_set[train_count]=image\n",
    "                y_train_set[train_count]=count\n",
    "                train_count+=1\n",
    "\n",
    "            # Filling validation set\n",
    "            for image_name in validation_images:\n",
    "    #             print(\"image_name\",image_name)\n",
    "                image=img.imread(class_name_path+image_name)\n",
    "    #             print(image.shape)\n",
    "                image=cv2.resize(image, dsize=(image_height,image_width), interpolation=cv2.INTER_CUBIC).reshape(image_height,image_width,n_channel)/255\n",
    "    #             print(image.shape)\n",
    "                x_validation_set[validation_count]=image\n",
    "                y_validation_set[validation_count]=count\n",
    "                validation_count+=1\n",
    "\n",
    "            # Filling test set\n",
    "            for image_name in test_images:\n",
    "    #             print(\"image_name\",image_name)\n",
    "                image=img.imread(class_name_path+image_name,)\n",
    "    #             print(image.shape)\n",
    "                image=cv2.resize(image, dsize=(image_height,image_width), interpolation=cv2.INTER_CUBIC).reshape(image_height,image_width,n_channel)/255\n",
    "    #             print(image.shape)\n",
    "                x_test_set[test_count]=image\n",
    "                y_test_set[test_count]=count\n",
    "                test_count+=1\n",
    "\n",
    "#         print(len(classes))\n",
    "    #     np.set_printoptions(threshold=np.inf)\n",
    "#         print(\"y_train_set\",y_train_set.shape,y_train_set)\n",
    "#         print(\"y_validation_set\",y_validation_set.shape,y_validation_set)\n",
    "#         print(\"y_test_set\",y_test_set.shape,y_test_set)\n",
    "\n",
    "        x_train_set,y_train_set = shuffle(x_train_set,y_train_set, random_state=0) # This shuffles two array altoghether \n",
    "        x_validation_set,y_validation_set = shuffle(x_validation_set,y_validation_set, random_state=0)\n",
    "        x_test_set,y_test_set = shuffle(x_test_set,y_test_set, random_state=0)\n",
    "        \n",
    "        with open(\"xy_train_set.pkl\",'wb') as f:\n",
    "            print(x_train_set.dtype)\n",
    "            pkl.dump((x_train_set,y_train_set),f)\n",
    "            \n",
    "        with open(\"xy_validation_set.pkl\",'wb') as f:\n",
    "            pkl.dump((x_validation_set,y_validation_set),f)\n",
    "            \n",
    "        with open(\"xy_test_set.pkl\",'wb') as f:\n",
    "            pkl.dump((x_test_set,y_test_set),f)\n",
    "        print(\"Data saved successfully\")\n",
    "        return x_train_set,y_train_set,x_validation_set,y_validation_set,x_test_set,y_test_set\n",
    "            \n",
    "    else:       \n",
    "        with open(\"xy_train_set.pkl\",'rb') as f:\n",
    "            x_train_set,y_train_set=pkl.load(f)\n",
    "        \n",
    "        with open(\"xy_validation_set.pkl\",'rb') as f:\n",
    "            x_validation_set,y_validation_set=pkl.load(f)\n",
    "            \n",
    "        with open(\"xy_test_set.pkl\",'rb') as f:\n",
    "            x_test_set,y_test_set=pkl.load(f)\n",
    "        \n",
    "        print(\"Data loaded successfully\")\n",
    "        return x_train_set,y_train_set,x_validation_set,y_validation_set,x_test_set,y_test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "x_train_set,y_train_set,x_validation_set,y_validation_set,x_test_set,y_test_set=load_dataset(isSave=False)\n",
    "# print(x_train_set,y_train_set,x_validation_set,y_validation_set,x_test_set,y_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 1 100 float32 float32 float32\n"
     ]
    }
   ],
   "source": [
    "m,image_height,image_width,n_channel=x_train_set.shape\n",
    "\n",
    "n_labels=100\n",
    "print(image_height,image_width,n_channel,n_labels,x_train_set.dtype,x_test_set.dtype,x_validation_set.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7a79fad2b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train_set[0].reshape(image_height,image_width),cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1100, 100)\n"
     ]
    }
   ],
   "source": [
    "# Applying hot_encode to every y array of training set,validation test and test set\n",
    "\n",
    "print(type(y_train_set))\n",
    "y_train_set=one_hot_encode(y_train_set,n_labels)\n",
    "y_validation_set=one_hot_encode(y_validation_set,n_labels)\n",
    "y_test_set=one_hot_encode(y_test_set,n_labels)\n",
    "print(y_train_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,shape=[None,image_height,image_width,n_channel])\n",
    "y=tf.placeholder(tf.float32,shape=[None,n_labels])\n",
    "drop_prob=tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(shape):\n",
    "    init_random_weight=tf.truncated_normal(shape,stddev=1.0)\n",
    "    return tf.Variable(init_random_weight)\n",
    "\n",
    "def initialize_bias(shape):\n",
    "    init_random_bias=tf.constant(0.1,shape=shape) # initialize all the weights with 0.1\n",
    "    return tf.Variable(init_random_bias)\n",
    "\n",
    "def conv2d(x,w):\n",
    "    return tf.nn.conv2d(x,w,strides=[1,1,1,1],padding=\"SAME\")\n",
    "\n",
    "def max_pooling_2d(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "def convolution_layer(input_x,shape):\n",
    "    w=initialize_weight(shape)\n",
    "    b=initialize_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x,w)+b)\n",
    "\n",
    "def normal_full_layer(input_layer,size):\n",
    "    input_size=int(input_layer.get_shape()[1]) # getting the number of neuron unit in next layer\n",
    "    w=initialize_weight([input_size,size])\n",
    "    b=initialize_bias([size])\n",
    "    return tf.matmul(input_layer,w)+b\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 50, 50, 1), dtype=float32)\n",
      "(?, 7, 7, 64)\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(x)\n",
    "# Layer 1\n",
    "convo_1=convolution_layer(x,shape=[5,5,1,32])\n",
    "max_pooling_1=tf.nn.max_pool(convo_1,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "# Layer 2\n",
    "convo_2=convolution_layer(max_pooling_1,shape=[5,5,32,48])\n",
    "max_pooling_2=tf.nn.max_pool(convo_2,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "# Layer 3\n",
    "convo_3=convolution_layer(max_pooling_2,shape=[5,5,48,64])\n",
    "max_pooling_3=tf.nn.max_pool(convo_3,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "last_shape=max_pooling_3.get_shape()\n",
    "print(last_shape)\n",
    "\n",
    "#Flattening\n",
    "convo_3_flat=tf.reshape(max_pooling_3,[-1,last_shape[-1]*last_shape[-2]*last_shape[-3]])\n",
    "\n",
    "#Fully connected layer 1\n",
    "full_layer_one=tf.nn.relu(normal_full_layer(convo_3_flat,1024))\n",
    "\n",
    "# dropout\n",
    "full_one_dropout=tf.nn.dropout(full_layer_one,keep_prob=drop_prob)\n",
    "\n",
    "#Final layer\n",
    "output_layer=normal_full_layer(full_one_dropout,n_labels)\n",
    "y_predict=output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "cross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train=optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(i,batch_size):\n",
    "    batch_x=x_train_set[i:i+batch_size]\n",
    "    batch_y=y_train_set[i:i+batch_size]\n",
    "    return batch_x,batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=500\n",
    "batch_size=50\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(steps):\n",
    "        batch_x,batch_y=next_batch(i,batch_size)\n",
    "        i=(i+batch_size)/m\n",
    "        sess.run(train,feed_dict={x: batch_x,y:batch_y,drop_prob:0.5})\n",
    "        if i%batch_size==0:\n",
    "            print(\"Step : {}\".format(i))\n",
    "            print(\"Accuracy\")\n",
    "            matches=tf.equal(tf.argmax(y_predict,1),tf.argmax(y,1))\n",
    "            accuracy=tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "            print(sess.run(accuracy,feed_dict={x:x_test_set,y:y_test_set,drop_prob:1.0}))\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4, 4], [5, 6, 6]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[[1,2],[3,4,4],[5,6,6]]\n",
    "l[1:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.divide(a,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.66666667, 1.        , 1.33333333])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
